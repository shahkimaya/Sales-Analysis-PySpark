{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "BBqYBxsyRWuA",
        "outputId": "478a9e02-1781-438c-9703-a90f2b079633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78f9a8268fa0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://36319d3d8f94:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DateType"
      ],
      "metadata": {
        "id": "B0gvgvaHyjqL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema=StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"order_date\", DateType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"source_order\", StringType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "2wtBgiqmyjnR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df=spark.read.format(\"csv\").option(\"InferSchema\", \"true\").schema(schema).load(\"/content/sample_data/sales.csv.txt\")\n",
        "display(sales_df)"
      ],
      "metadata": {
        "id": "55gTP1oAyjkP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2a04ee2-8b43-4581-ae71-0c10d3cc1876"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[product_id: int, customer_id: string, order_date: date, location: string, source_order: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YSP_cYKyjhC",
        "outputId": "58a26afa-2cad-412b-cec9-914662ac4033"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+----------+--------+------------+\n",
            "|product_id|customer_id|order_date|location|source_order|\n",
            "+----------+-----------+----------+--------+------------+\n",
            "|         1|          A|2023-01-01|   India|      Swiggy|\n",
            "|         2|          A|2022-01-01|   India|      Swiggy|\n",
            "|         2|          A|2023-01-07|   India|      Swiggy|\n",
            "|         3|          A|2023-01-10|   India|  Restaurant|\n",
            "|         3|          A|2022-01-11|   India|      Swiggy|\n",
            "|         3|          A|2023-01-11|   India|  Restaurant|\n",
            "|         2|          B|2022-02-01|   India|      Swiggy|\n",
            "|         2|          B|2023-01-02|   India|      Swiggy|\n",
            "|         1|          B|2023-01-04|   India|  Restaurant|\n",
            "|         1|          B|2023-02-11|   India|      Swiggy|\n",
            "+----------+-----------+----------+--------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deriving year, Month, Quarter"
      ],
      "metadata": {
        "id": "8yfiQmqS0OJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, year, quarter"
      ],
      "metadata": {
        "id": "3eMQZpLFyjeO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df=sales_df.withColumn(\"order_year\", year(sales_df.order_date))\n",
        "sales_df=sales_df.withColumn(\"order_month\", month(sales_df.order_date))\n",
        "sales_df=sales_df.withColumn(\"order_quarter\", quarter(sales_df.order_date))\n"
      ],
      "metadata": {
        "id": "Bo1JdN0iyjbE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.show(18)"
      ],
      "metadata": {
        "id": "uLD_VrveyjYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fa2c73-8783-4845-ef00-4ce6a344ec1c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
            "|product_id|customer_id|order_date|location|source_order|order_year|order_month|order_quarter|\n",
            "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
            "|         1|          A|2023-01-01|   India|      Swiggy|      2023|          1|            1|\n",
            "|         2|          A|2022-01-01|   India|      Swiggy|      2022|          1|            1|\n",
            "|         2|          A|2023-01-07|   India|      Swiggy|      2023|          1|            1|\n",
            "|         3|          A|2023-01-10|   India|  Restaurant|      2023|          1|            1|\n",
            "|         3|          A|2022-01-11|   India|      Swiggy|      2022|          1|            1|\n",
            "|         3|          A|2023-01-11|   India|  Restaurant|      2023|          1|            1|\n",
            "|         2|          B|2022-02-01|   India|      Swiggy|      2022|          2|            1|\n",
            "|         2|          B|2023-01-02|   India|      Swiggy|      2023|          1|            1|\n",
            "|         1|          B|2023-01-04|   India|  Restaurant|      2023|          1|            1|\n",
            "|         1|          B|2023-02-11|   India|      Swiggy|      2023|          2|            1|\n",
            "|         3|          B|2023-01-16|   India|      zomato|      2023|          1|            1|\n",
            "|         3|          B|2022-02-01|   India|      zomato|      2022|          2|            1|\n",
            "|         3|          C|2023-01-01|   India|      zomato|      2023|          1|            1|\n",
            "|         1|          C|2023-01-01|      UK|      Swiggy|      2023|          1|            1|\n",
            "|         6|          C|2022-01-07|      UK|      zomato|      2022|          1|            1|\n",
            "|         3|          D|2023-02-16|      UK|  Restaurant|      2023|          2|            1|\n",
            "|         5|          D|2022-02-01|      UK|      zomato|      2022|          2|            1|\n",
            "|         3|          E|2023-02-01|      UK|  Restaurant|      2023|          2|            1|\n",
            "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
            "only showing top 18 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema=StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"price\", StringType(), True),\n",
        "])\n",
        "menu_df=spark.read.format(\"csv\").option(\"InferSchema\", \"true\").schema(schema).load(\"/content/sample_data/menu.csv.txt\")\n",
        "display(menu_df)"
      ],
      "metadata": {
        "id": "HTcGP7LJyjVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdd7fe57-2af0-4196-f6c3-1039e57442c7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[product_id: int, product_name: string, price: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "menu_df.show(10)"
      ],
      "metadata": {
        "id": "yMOOJgBcyjR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be504c9b-7e8e-4e7f-bea4-cafd9325d738"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----+\n",
            "|product_id|product_name|price|\n",
            "+----------+------------+-----+\n",
            "|         1|       PIZZA|  100|\n",
            "|         2|     Chowmin|  150|\n",
            "|         3|    sandwich|  120|\n",
            "|         4|        Dosa|  110|\n",
            "|         5|     Biryani|   80|\n",
            "|         6|       Pasta|  180|\n",
            "+----------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Amount Spent by each **Customer**"
      ],
      "metadata": {
        "id": "6XSxlqyrOA2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PySpark"
      ],
      "metadata": {
        "id": "Watvs3vfSdMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_amount_spent=(sales_df.join(menu_df,'product_id').groupby('customer_id').agg({'price':'sum'}).orderBy('customer_id'))\n",
        "total_amount_spent.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiYogBoPN_hZ",
        "outputId": "bfc130a1-2886-4647-d1a9-864b39873a78"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|customer_id|sum(price)|\n",
            "+-----------+----------+\n",
            "|          A|    4260.0|\n",
            "|          B|    4440.0|\n",
            "|          C|    2400.0|\n",
            "|          D|    1200.0|\n",
            "|          E|    2040.0|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using SQL"
      ],
      "metadata": {
        "id": "XvLhJuenSgOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.createOrReplaceTempView(\"Sales\")\n",
        "menu_df.createOrReplaceTempView(\"Menu\")"
      ],
      "metadata": {
        "id": "RM_AD3HVN_eR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.customer_id, sum(m.price) as Price from Sales s, Menu m where s.product_id == m.product_id group by 1 order by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLnoY0lHN_bK",
        "outputId": "bb394676-f189-4e82-e276-3ebedb27c297"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+\n",
            "|customer_id|Price |\n",
            "+-----------+------+\n",
            "|A          |4260.0|\n",
            "|B          |4440.0|\n",
            "|C          |2400.0|\n",
            "|D          |1200.0|\n",
            "|E          |2040.0|\n",
            "+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Amount Spent by Each Food Category"
      ],
      "metadata": {
        "id": "iID0cxdsVmoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PySpark"
      ],
      "metadata": {
        "id": "nOAvxkO3Wxv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_amount_spent=(sales_df.join(menu_df,'product_id').groupby('product_name').agg({'price':'sum'}).orderBy('product_name'))\n",
        "total_amount_spent.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNlkUawkN_X-",
        "outputId": "a86e2dfe-0929-44fb-94db-ffe23f483a13"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|product_name|sum(price)|\n",
            "+------------+----------+\n",
            "|     Biryani|     480.0|\n",
            "|     Chowmin|    3600.0|\n",
            "|        Dosa|    1320.0|\n",
            "|       PIZZA|    2100.0|\n",
            "|       Pasta|    1080.0|\n",
            "|    sandwich|    5760.0|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using SQL"
      ],
      "metadata": {
        "id": "sovwBVNxXBGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select m.product_name, sum(m.price) as Price from Sales s, Menu m where s.product_id == m.product_id group by 1 order by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GpRsn9-XCAY",
        "outputId": "9274e46c-0370-4021-9017-7ee3b182c5af"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+\n",
            "|product_name|Price |\n",
            "+------------+------+\n",
            "| Biryani    |480.0 |\n",
            "| Chowmin    |3600.0|\n",
            "| Dosa       |1320.0|\n",
            "| PIZZA      |2100.0|\n",
            "| Pasta      |1080.0|\n",
            "| sandwich   |5760.0|\n",
            "+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Amount of Sales in Each Month"
      ],
      "metadata": {
        "id": "iI0ZCSBjXPLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PySpark"
      ],
      "metadata": {
        "id": "qRahwXTOXYrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=(sales_df.join(menu_df,'product_id').groupby('order_month').agg({'price':'sum'}).orderBy('order_month'))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igxm2yY_N_U3",
        "outputId": "49bf37fb-23a3-4834-8b64-8a8c73ded162"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|order_month|sum(price)|\n",
            "+-----------+----------+\n",
            "|          1|    2960.0|\n",
            "|          2|    2730.0|\n",
            "|          3|     910.0|\n",
            "|          5|    2960.0|\n",
            "|          6|    2960.0|\n",
            "|          7|     910.0|\n",
            "|         11|     910.0|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SQL"
      ],
      "metadata": {
        "id": "2Jnv47doXozk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.order_month, sum(m.price) as Price from Sales s, Menu m where s.product_id == m.product_id group by 1 order by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZMfwRWKN_Rs",
        "outputId": "ebd2186c-8553-4d50-fba2-b8248cebfb8b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+\n",
            "|order_month|Price |\n",
            "+-----------+------+\n",
            "|1          |2960.0|\n",
            "|2          |2730.0|\n",
            "|3          |910.0 |\n",
            "|5          |2960.0|\n",
            "|6          |2960.0|\n",
            "|7          |910.0 |\n",
            "|11         |910.0 |\n",
            "+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yearly Sales"
      ],
      "metadata": {
        "id": "e89JLOzXX1Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using PySpark"
      ],
      "metadata": {
        "id": "dvedytn1X5LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=(sales_df.join(menu_df,'product_id').groupby('order_year').agg({'price':'sum'}).orderBy('order_year'))\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "gWcHdVFFyjPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6012433-8860-49be-fbae-403f5ac9fa04"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|order_year|sum(price)|\n",
            "+----------+----------+\n",
            "|      2022|    4350.0|\n",
            "|      2023|    9990.0|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SQL"
      ],
      "metadata": {
        "id": "jskPh4cGYC-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.order_year, sum(m.price) as Price from Sales s, Menu m where s.product_id == m.product_id group by 1 order by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "id": "dQcqKeppwYHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7b3c09-82c4-405d-b1a4-f736cf4101ba"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|order_year|Price |\n",
            "+----------+------+\n",
            "|2022      |4350.0|\n",
            "|2023      |9990.0|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quartely Sales"
      ],
      "metadata": {
        "id": "xsGDbeuMYKr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=(sales_df.join(menu_df,'product_id').groupby('order_quarter').agg({'price':'sum'}).orderBy('order_quarter'))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-dejzWLYKWu",
        "outputId": "c2e0bd5d-96cb-4718-e79a-fb1b00c333ff"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|order_quarter|sum(price)|\n",
            "+-------------+----------+\n",
            "|            1|    6600.0|\n",
            "|            2|    5920.0|\n",
            "|            3|     910.0|\n",
            "|            4|     910.0|\n",
            "+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.order_quarter, sum(m.price) as Price from Sales s, Menu m where s.product_id == m.product_id group by 1 order by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO7HJTnRYKUV",
        "outputId": "e94f2f2a-f1cd-4b0e-c3a8-6ce9176b94b8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+\n",
            "|order_quarter|Price |\n",
            "+-------------+------+\n",
            "|1            |6600.0|\n",
            "|2            |5920.0|\n",
            "|3            |910.0 |\n",
            "|4            |910.0 |\n",
            "+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How many times each product was purchased"
      ],
      "metadata": {
        "id": "_Fe0qAtwYrBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=(sales_df.join(menu_df,'product_id').groupby('product_id','product_name').agg({'product_id':'count'}).drop('product_id')) #.orderBy('order_quarter'))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seRALuvVYKRs",
        "outputId": "4b418813-fc05-482e-eddc-6ce77eccfaed"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------------+\n",
            "|product_name|count(product_id)|\n",
            "+------------+-----------------+\n",
            "|     Biryani|                6|\n",
            "|    sandwich|               48|\n",
            "|        Dosa|               12|\n",
            "|     Chowmin|               24|\n",
            "|       PIZZA|               21|\n",
            "|       Pasta|                6|\n",
            "+------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.product_id, m.product_name, count(s.product_id) as product_count from Sales s, Menu m where s.product_id == m.product_id group by 1,2 order by 3 desc\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOn0ZtLmYKPV",
        "outputId": "e7d6f8f1-e7bf-4bda-cf81-aac3f4f97d82"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|product_name|product_count|\n",
            "+----------+------------+-------------+\n",
            "|3         | sandwich   |48           |\n",
            "|2         | Chowmin    |24           |\n",
            "|1         | PIZZA      |21           |\n",
            "|4         | Dosa       |12           |\n",
            "|5         | Biryani    |6            |\n",
            "|6         | Pasta      |6            |\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency of Customer Visited to Restaurant"
      ],
      "metadata": {
        "id": "vDedCorSbQqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "df=(sales_df.filter(sales_df.source_order=='Restaurant').groupBy('customer_id').agg(countDistinct('order_date')))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zq61LWzYKM-",
        "outputId": "26b3b9ce-a0ea-40e0-df11-d0c3080259e2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------------+\n",
            "|customer_id|count(order_date)|\n",
            "+-----------+-----------------+\n",
            "|          E|                5|\n",
            "|          B|                6|\n",
            "|          D|                1|\n",
            "|          C|                3|\n",
            "|          A|                6|\n",
            "+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select customer_id, count (distinct order_date) as frequency from sales where source_order='Restaurant' group by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq0NBdtgYKKo",
        "outputId": "83838599-808f-4b96-c38d-d08096166b36"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "|customer_id|frequency|\n",
            "+-----------+---------+\n",
            "|E          |5        |\n",
            "|B          |6        |\n",
            "|D          |1        |\n",
            "|C          |3        |\n",
            "|A          |6        |\n",
            "+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Sales by Each Country"
      ],
      "metadata": {
        "id": "t1a5xwg_c-sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=(sales_df.join(menu_df,'product_id').groupby('location').agg({'price':'sum'}))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DODcD6zjYKHh",
        "outputId": "4ee1d294-e14a-4574-cfc0-b361431a93f5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|location|sum(price)|\n",
            "+--------+----------+\n",
            "|   India|    4860.0|\n",
            "|     USA|    2460.0|\n",
            "|      UK|    7020.0|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.location, sum (m.price) as total_sales from sales s join menu m on s.product_id=m.product_id group by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdXcxiXCYKFC",
        "outputId": "78a39ace-8910-4056-d0d4-b8f7286b504e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|location|total_sales|\n",
            "+--------+-----------+\n",
            "|India   |4860.0     |\n",
            "|USA     |2460.0     |\n",
            "|UK      |7020.0     |\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Sales by Order Source"
      ],
      "metadata": {
        "id": "xWTgTyzHdk8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=(sales_df.join(menu_df,'product_id').groupby('source_order').agg({'price':'sum'}))\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "JGWrSDjxwYD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a2e60d-095e-4cc3-fa71-7c8e79e6a4cd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|source_order|sum(price)|\n",
            "+------------+----------+\n",
            "|      zomato|    4920.0|\n",
            "|      Swiggy|    6330.0|\n",
            "|  Restaurant|    3090.0|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinDF = spark.sql(\"select s.source_order, sum (m.price) as total_sales from sales s join menu m on s.product_id=m.product_id group by 1\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "id": "QMpTwjntwYAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40b89da-a1e4-4620-891b-2769138a4af9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "|source_order|total_sales|\n",
            "+------------+-----------+\n",
            "|zomato      |4920.0     |\n",
            "|Swiggy      |6330.0     |\n",
            "|Restaurant  |3090.0     |\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0qD_RzalPxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}